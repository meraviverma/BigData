Issue1:
Multiple sources found for csv (org.apache.spark.sql.execution.datasources.csv.CSVFileFormat,com.databricks.spark.csv.DefaultSource15)

Resolution:
-----------
use  csv format as below for this issue
val orderscsv=spark.read.format("org.apache.spark.sql.execution.datasources.csv.CSVFileFormat")
                  .option("header", "false")
                  .option("inferSchema", "true")
                  .load("c:/Users/rv00451128/IdeaProjects/MyFirst/orders_csv - Copy.csv")
                  .toDF("order_id","order_date","order_customer_id","order_status")
 Issue2:
 --------------
 Error:(53, 25) Unable to find encoder for type stored in a Dataset.  Primitive types (Int, String, etc) and
    Product types (case classes) are supported by importing spark.implicits._  Support for serializing other types will be added in
    future releases
 Resolution:
 ---------------
 //Declare case class outside the object scope. Declare case class outside main.

   // case class Order(order_id: Int,order_date:String,order_customer_id:Int,order_status:String)
     val orderds=spark.read.textFile("c:/Users/rv00451128/IdeaProjects/MyFirst/orders.txt")
      .map(o=>{
        val a=o.split(",")
        Order(a(0).toInt,a(1),a(2).toInt,a(3))
      }).as[Order]

Issue3:
--------------------
Seeting up win utils file in Intellije and solving error winutils error:
java.io.IOException: Could not locate executable D:\software\winutils-master\hadoop-2.8.1\bin\winutils.exe in the Hadoop binaries

Download winutilsfile and keep in folder . Example D:/software/hadoop/bin
and set property in your code.
 System.setProperty("hadoop.home.dir", "D:\\software\\winutils-master\\hadoop")
 
 def main(arg: Array[String]){
    System.setProperty("hadoop.home.dir", "D:\\software\\winutils-master\\hadoop")
    val spark=SparkSession
      .builder()
      .appName("Class11")
      //.config("spark.some.config.option", "some-value")
      .master("local")
      .getOrCreate()
      }
    And you are ready to go
    
    
Issue4:
------------------------------------
ava.lang.ClassCastException: com.datastax.driver.core.DefaultResultSetFuture cannot be cast 
to shade.com.datastax.spark.connector.google.common.util.concurrent.ListenableFuture

Remove below dependency:
<dependency>
      <groupId>com.datastax.cassandra</groupId>
      <artifactId>cassandra-driver-core</artifactId>
      <version>3.5.1</version>
    </dependency>-->
    
 Use Only below dependency:
 ------------------------------
 <dependencies>
    <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core -->
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-core_${scala.tools.version}</artifactId>
      <version>2.3.1</version>
    </dependency>

    <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql -->
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-sql_${scala.tools.version}</artifactId>
      <version>2.3.1</version>
    </dependency>
    
     <dependency>
      <groupId>com.datastax.spark</groupId>
      <artifactId>spark-cassandra-connector_${scala.tools.version}</artifactId>
      <version>2.3.1</version>
    </dependency>


    <dependency>
      <groupId>org.scala-lang</groupId>
      <artifactId>scala-library</artifactId>
      <version>${scala.version}</version>
    </dependency>

  </dependencies>
  
  Issue 4
  ----------------
  Exception in thread "main" java.lang.NoSuchMethodError: scala.Predef$.refArrayOps
  ([Ljava/lang/Object;)Lscala/collection/mutable/ArrayOps;
  Use scala 2.11
  I had SDK in global libraries with a different version of Scala(IntelliJ IDEA).
File -> Project Structure -> Global libraries -> Remove SDK -> Rebuild. It fixed the Exception for me.


Issue 5
------------------
reading json gives error. When printing schema it gives error like:
root
 |-- _corrupt_record: string (nullable = true)

()
 Don't follow this way to import:
 val student =sc.read.json("D:\\mypro\\spark\\student_detai.json")
 
 Follow:
 val student =sc.read.option("multiline","true").json("D:\\mypro\\spark\\student_detai.json")
 
 Issue 6
 ------------------
 Running hadoop mapreduce code in windows
 
 Exception in thread “main” java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
 
 https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin
 
 After putting haddop.dll and winutils in hadoop/bin folder and adding the folder of hadoop to PATH, 
 we also need to put hadoop.dll into the C:\Windows\System32 folder
 
 https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io
 
 Issue 7
 ------------------------
 Issue while writing to avro file format.
 
 student.write.format("com.databricks.spark.avro").save("D:\\mypro\\spark\\avro-data")
 
 java.lang.AbstractMethodError: org.apache.spark.sql.execution.datasources.OutputWriter.write
 (Lorg/apache/spark/sql/catalyst/InternalRow;)V
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:392)
	at 
  
  Solution:
  -------------
  Use this dependency:
  <dependency>
            <groupId>com.databricks</groupId>
            <artifactId>spark-avro_2.11</artifactId>
            <version>4.0.0</version>
        </dependency>
 


