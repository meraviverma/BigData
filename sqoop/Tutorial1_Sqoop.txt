Sqoop Import Command
--------------------------------------------------------------
sqoop import
--connect jdbc:mysql://mysql.example.com/sqoop \
--username sqoop \
--password sqoop \
--table cities \

In sqoop there is no reducer. There is only mapper. It will sqoop all the records from table.
By default it is 4 mapper.Each mapper will copy subset of data.
It get total count of no of record.It get min of id and max(id) and then divide it by 4.No of splits is 4.

------------------------------------------------------------------------
CASE1: What if you want to change path where sqoop should import
------------------------------------------------------------------------
We need to use --target-dir
sqoop import
--connect jdbc:mysql://mysql.example.com/sqoop \
--username sqoop \
--password sqoop \
--table cities \
--target-dir /etl/input/cities

NOTE: Make sure that directory in which you are importing is new directory.Otherwise it will throw error.

Internally it creates a jar file and then run it in hadoop

-------------------------------------------------------------------------------------------------------
CASE2:Suppose you don't want to specify target-dir every time so we can use data warehouse directory.
It will import into that directory with the table name as directory name.
-------------------------------------------------------------------------------------------------------
--warehouse-dir

sqoop import
--connect jdbc:mysql://mysql.example.com/sqoop \
--username sqoop \
--password sqoop \
--table cities \
--warehouse-dir /etl/input/

By default, Sqoop will create a directory with the same name as the imported table inside
your home directory on HDFS and import all data there.

-------------------------------------------------------------------------------------------------------
CASE3: Suppose you want to import only subset of data.
-------------------------------------------------------------------------------------------------------
"--where"

sqoop import
--connect jdbc:mysql://mysql.example.com/sqoop \
--username sqoop \
--password sqoop \
--table cities \
--where "country = 'IND'"

Advanced functions could lock certain tables, preventing Sqoop from transferring data in parallel. This will
adversely affect transfer performance. For efficient advanced filtering, run the filtering query on your database prior to 
import, 
save its output to a temporary table and run Sqoop to import the temporary table into Hadoop

-------------------------------------------------------------------------------------------------------
CASE4: Protect password
-------------------------------------------------------------------------------------------------------
1)You can specify password prompt [-P]
2)you can save password in a password file.[--password-file]

sqoop import \
--connect jdbc:mysql://mysql.example.com/sqoop \
--username sqoop \
--table cities \
-P

sqoop import \
--connect jdbc:mysql://mysql.example.com/sqoop \
--username sqoop \
--table cities \
--password-file my-sqoop-password

set the file’s permissions to 400,so no one else can open the file and fetch the password.
echo "my-secret-password" > sqoop.password
hadoop dfs -put sqoop.password /user/$USER/sqoop.password
hadoop dfs -chown 400 /user/$USER/sqoop.password

rm sqoop.password
sqoop import --password-file /user/$USER/sqoop.password

When using a text editor to manually
edit the password file, be sure not to introduce extra empty lines at the end of the file.
-------------------------------------------------------------------------------------------------------
CASE5:Using File format other than CSV
-------------------------------------------------------------------------------------------------------
Default is csv file format.
--as-sequencefile
--as-avrodatafile

sqoop import \
--connect jdbc:mysql://mysql.example.com/sqoop \
--username sqoop \
--password sqoop \
--table cities \
--as-sequencefile

sqoop import \
--connect jdbc:mysql://mysql.example.com/sqoop \
--username sqoop \
--password sqoop \
--table cities \
--as-avrodatafile

generic data serialization. It use binary behind the scene. Json string all schema is encode in avro

Binary Format advantage:
---------------------------
Binary formats have a few benefits over that of text files. First, binary formats are a natural fit for storing binary values like images or PDF documents. They are also more suited for storing text data if the data itself contains characters that are otherwise 
used as separators in the text file.
Binary Format Disadvantage:
----------------------------
In order to access the binary data, you need to implement extra functionality or load special libraries in your application.

Sequence File Format
------------------------------
1)The SequenceFile is a special Hadoop file format that is used for storing objects and implements the Writable interface. 
This format was customized for MapReduce, and thus it expects that each record will consist of two parts: key and value. 
2)Sqoop does not have the concept of key-value pairs and thus uses an empty object called NullWritable
in place of the value. For the key, Sqoop uses the generated class.
3)For convenience, this generated class is copied to the directory where Sqoop is executed. You will need to
integrate this generated class to your application if you need to read a Sqoop-generated SequenceFile.

Avro File Format
------------------------------
1)Avro is a very generic system that can store any arbitrary data structures. It uses a concept called
schema to describe what data structures are stored within the file.
2)The schema is usually encoded as a JSON string so that it’s decipherable by the human eye. Sqoop will generate
the schema automatically based on the metadata information retrieved from the database server and will retain the schema in each generated file.

-------------------------------------------------------------------------------------------------------
CASE6: Compressing Imported Data	--compress --compression-codec
-------------------------------------------------------------------------------------------------------
You want to decrease the overall size occupied on HDFS by using compression for generated files.
sqoop import \
--connect jdbc:mysql://mysql.example.com/sqoop \
--username sqoop \
--table cities \
--compress

By Default compression is GZip codec. And all files will end up with a .gz extension.

We can choose any other codec using --compression-codec parameter.
We can use BZip2 insstead of GZip (file will end as .bz2)

--compression-codec org.apache.hadoop.io.compress.BZip2Codec

NOTE: Before using any compression codec make sure that desired codec is properly installed and configured 
across all nodes in cluster.

if in the mapred-site.xml file, the property mapred.output.compress is set to false with the final flag, then
Sqoop won’t be able to compress the output files even when you call it with the --compress parameter.

sqoop import \
--connect jdbc:mysql://mysql.example.com/sqoop \
--username sqoop \
--table cities \
--compress
--compression-codec org.apache.hadoop.io.compress.SnappyCodec

-------------------------------------------------------------------------------------------------------
Q)What is Direct mode in sqoop?	--direct
-------------------------------------------------------------------------------------------------------
For some databases you can take advantage of the direct mode by using the --direct

sqoop import \
--connect jdbc:mysql://mysql.example.com/sqoop \
--username sqoop \
--table cities \
--direct

Rather than using the JDBC interface for transferring data, the direct mode delegates the job of transferring data to the native utilities provided by the database vendor.

In the case of MySQL, the mysqldump and mysqlimport will be used for retrieving data from the database server or moving data back.
In the case of PostgreSQL, Sqoop will take advantage of the pg_dump utility to import data.

Using native utilities will greatly improve performance, as they are optimized to provide the best possible transfer speed while 
putting less burden on the database server.

Disadvantage:
-----------------
Because all data transfer operations are performed inside generated MapReduce jobs and because the data transfer is being deferred to native utilities in direct mode, you will need to make sure that those native utilities are available on all of your Hadoop 
TaskTracker nodes. For example, in the case of MySQL, each node hosting a TaskTracker service needs to have both mysqldump and mysqlimport utilities installed.

As the native utilities usually produce text output, binary formats like SequenceFile or Avro won’t work.

------------------------------------------------------
CASE7:CONTTROLLING PARALLELISM
------------------------------------------------------
--num-mappers

sqoop import \
--connect jdbc:mysql://mysql.example.com/sqoop \
--username sqoop \
--password sqoop \
--table cities \
--num-mappers 10

If your data set is very small, Sqoop might resort to using a smaller number of mappers. For example, if you’re transferring only 
4 rows yet set --num-mappers to 10 mappers, only 4 mappers will be used.

Increasing the number of mappers beyond this point won’t lead to faster job completion; in fact, it will have the opposite effect 
as your database server spends more time doing context switching rather than serving data.
------------------------------------------------------
case8: Encoding NULL Values
------------------------------------------------------
Sqoop encodes database NULL values using the null string constant.

sqoop import \
--connect jdbc:mysql://mysql.example.com/sqoop \
--username sqoop \
--password sqoop \
--table cities \
--null-string '\\N' \
--null-non-string '\\N'

For text-based columns that are defined with type VARCHAR, CHAR, NCHAR, TEXT, and a few others, you can override the default substitution string using the parameter --null-string.

------------------------------------------------------
CASE9:Importing All Your Tables
------------------------------------------------------
sqoop import-all-tables \
--connect jdbc:mysql://mysql.example.com/sqoop \
--username sqoop \
--password sqoop

you can use the parameter --excludetables that accepts a comma-separated list of table names that should be excluded from the bulk import.

sqoop import-all-tables \
--connect jdbc:mysql://mysql.example.com/sqoop \
--username sqoop \
--password sqoop \
--exclude-tables cities,countries

For example, you can’t use the parameter --target-dir, as that would instruct Sqoop to import all tables into the same directory, resulting in a total file mess on HDFS.

Using the --warehouse-dir parameter is fine, as this parameter can be easily used for all imported tables.

------------------------------------------------------
CASE10: INCREMENTAL IMPORT --incremental --check-column --last-value
------------------------------------------------------
--incremental parameter

Two types of incremental import:
1)append
2)last modified


The parameter value will be type of incremental import.

Incremental import also requires two additional parameters: --check-column indicates
a column name that should be checked for newly appended data, and --last-value
contains the last value that successfully imported into Hadoop.

sqoop import \
--connect jdbc:mysql://mysql.example.com/sqoop \
--username sqoop \
--password sqoop \
--table visits \
--incremental append \
--check-column id \
--last-value 1

In sqoop we have an option of sqoop job --create, so next time when you use append mode you 
don't need to specify the last-value.Sqoop will store that value in metastore

sqoop import \
--connect jdbc:mysql://mysql.example.com/sqoop \
--username sqoop \
--password sqoop \
--table visits \
--incremental lastmodified \
--check-column last_update_date \
--last-value "2013-05-22 01:01:01"

The incremental mode lastmodified requires a column holding a date value (suitable
types are date, time, datetime, and timestamp) containing information as to when each
row was last updated.

------------------------------------------------------
CASE11: Preserving last imported value
------------------------------------------------------
We need to remember last imported value in sqoop for incremental import.

We can take advantage of sqoop built in metastore that allows you to save all parameter for later use.

sqoop job \
--create visits \

-- \
import \
--connect jdbc:mysql://mysql.example.com/sqoop \
--username sqoop \
--password sqoop \
--table visits \
--incremental append \
--check-column id \
--last-value 0

Sqoop that allows you to retain your job
definitions and to easily run them anytime. Each saved job has a logical name that is
used for referencing. You can list all retained jobs using the --list parameter:

sqoop job --list

You can remove the old job definitions that are no longer needed with the --delete
parameter, for example:

sqoop job --delete visits

And finally, you can also view content of the saved job definitions using the --show
parameter, for example:

sqoop job --show visits

------------------------------------------------------
CASE12: FREE FORM QUERY
------------------------------------------------------
cases where you need to import data from more than one table or where you
need to customize the transferred data by calling various database functions.

sqoop import \
--connect jdbc:mysql://mysql.example.com/sqoop \
--username sqoop \
--password sqoop \
--query 'SELECT normcities.id, \
countries.country, \
normcities.city \
FROM normcities \
JOIN countries USING(country_id) \
WHERE $CONDITIONS' \
--split-by id \
--target-dir cities

In addition to the --query parameter, you need
to specify the --split-by parameter with the column that should be used for slicing
your data into multiple parallel tasks. This parameter usually automatically defaults to
the primary key of the main table. The third required parameter is --target-dir, which
specifies the directory on HDFS where your data should be stored.

NOTE:"The free-form query import can’t be used in conjunction with the \--warehouse-dir parameter."

$CONDITIONS is replaced with condition of mapper.Sqoop
will automatically substitute this placeholder with the generated conditions specifying
which slice of data should be transferred by each individual task.

While you could skip $CONDITIONS by forcing Sqoop to run only one job using the --num-mappers 1 parameter.

sqoop import \
--connect jdbc:mysql://mysql.example.com/sqoop \
--username sqoop \
--password sqoop \
--query 'SELECT normcities.id, \
countries.country, \
normcities.city \
FROM normcities \
JOIN countries USING(country_id) \
WHERE  id <=20
AND $CONDITIONS' \
--split-by id \
--target-dir cities

------------------------------------------------------
CASE13: BOUNDRY QUERY
------------------------------------------------------
sqoop import \
--connect jdbc:mysql://mysql.example.com/sqoop \
--username sqoop \
--password sqoop \
--query 'SELECT normcities.id, \
countries.country, \
normcities.city \
FROM normcities \
JOIN countries USING(country_id) \
WHERE $CONDITIONS' \
--split-by id \
--target-dir cities \
--boundary-query "select min(id), max(id) from normcities"

------------------------------------------------------
Q) How will you sqoop if you don't have any primary key?
------------------------------------------------------
Ans: We can use no. of mapper=1 or we can use split-by

sqoop import \
    --connect jdbc:mysql://localhost/test_db \
    --username root \
    --password **** \
    --table <RDBMS-Table-name> \
    --target-dir /user/root/user_data \
    --hive-import \ 
    --hive-table <hive-table-name> \
    --create-hive-table \
    -m 1 (or) --split-by <RDBMS-Column>
------------------------------------------------------
Q)Sqoop import to Hive table. Sqoop to import your data directly into Hive.
------------------------------------------------------
sqoop import 
--connect jdbc:mysql://localhost/test
--table x1
-m 1
--hive-import

Sometimes the default mapping doesn’t work correctly for your needs; in those cases,
you can use the parameter 
--map-column-hive to override it.

sqoop import \
...
--hive-import \
--map-column-hive id=STRING,price=DECIMAL

--hive-overwrite

1) The biggest advantage of using Sqoop for populating tables in Hive is that it can automatically populate the metadata for you. 
If the table in Hive does not exist yet, Sqoop will simply create it based on the metadata fetched for your table or query. 
If the table already exists, Sqoop will import data into the existing table.

2)If you’re creating a new Hive table, Sqoop will convert the data types of each column from your source table to a type compatible 
with Hive. Usually this conversion is straightforward: for example,JDBC types VARCHAR, CHAR, and other string-based types are all 
mapped to Hive STRING.

3) Sometimes the default mapping doesn’t work correctly for your needs; in those cases,you can use the parameter --map-column-hive 
to override it.

4) During a Hive import, Sqoop will first do a normal HDFS import to a temporary location. After a successful import, Sqoop 
generates two queries: one for creating a table and another one for loading the data from a temporary location. You can specify 
any temporary location using either the --target-dir or --warehouse-dir parameter. It’s important not to use Hive’s warehouse 
directory (usually /user/hive/warehouse) for the temporary location, as it may cause issues with loading data in the second step.

5)If your table already exists and contains data, Sqoop will append to the newly imported data. You can change this behavior by using the parameter --hive-overwrite, which will instruct Sqoop to truncate an existing Hive table and load only the newly imported
one.


